{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Unsupervised Ensemble Machine Learning (SUEML)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import libraries, set settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas, numpy, matplotlib, and seaborn libraries\n",
    "#dataframe\n",
    "import pandas as pd\n",
    "#numerical and statistical\n",
    "import numpy as np\n",
    "#visualization\n",
    "import matplotlib.pyplot as plt\n",
    "#visualization extension\n",
    "import seaborn as sns                   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress FutureWarning messages\n",
    "import warnings             \n",
    "warnings.simplefilter(action='ignore', category=FutureWarning) \n",
    "# Ensure all numbers are displayed as regular numeric format\n",
    "pd.set_option('display.float_format', lambda x: '%.0f' % x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load data and analyze data\n",
    "df = pd.read_csv('example.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 17 columns):\n",
      " #   Column                Non-Null Count  Dtype \n",
      "---  ------                --------------  ----- \n",
      " 0   checking_balance      1000 non-null   object\n",
      " 1   months_loan_duration  1000 non-null   int64 \n",
      " 2   credit_history        1000 non-null   object\n",
      " 3   purpose               1000 non-null   object\n",
      " 4   amount                1000 non-null   int64 \n",
      " 5   savings_balance       1000 non-null   object\n",
      " 6   employment_duration   1000 non-null   object\n",
      " 7   percent_of_income     1000 non-null   int64 \n",
      " 8   years_at_residence    1000 non-null   int64 \n",
      " 9   age                   1000 non-null   int64 \n",
      " 10  other_credit          1000 non-null   object\n",
      " 11  housing               1000 non-null   object\n",
      " 12  existing_loans_count  1000 non-null   int64 \n",
      " 13  job                   1000 non-null   object\n",
      " 14  dependents            1000 non-null   int64 \n",
      " 15  phone                 1000 non-null   object\n",
      " 16  default               1000 non-null   object\n",
      "dtypes: int64(7), object(10)\n",
      "memory usage: 132.9+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Display columns, datatype, count, null count, \n",
    "display(df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "display('null #', df.isnull().sum(),\n",
    "        'null %', (df.isnull().sum()/(len(df)))*100 )\n",
    "display('df.shape', df.shape, 'df.describe().T', df.describe().T, \n",
    "        'df.nunique()', df.nunique(), 'df.duplicated().sum()', df.duplicated().sum(), \n",
    "        'df.head', df.head(), 'df.tail()', df.tail(), 'df.sample(10)', df.sample(10))\n",
    "# df.target.value_counts()\n",
    "print(f'unique crop: {df.target.nunique()}\\n{df.target.value_counts()}')\n",
    "# df.rename()\n",
    "df = df.rename(columns={“name”:”new_name”}, axis=1)\n",
    "# fill data\n",
    "df['col_name'].fillna(data['col_name'].mode()[0], inplace=True) # impute values\n",
    "# drop data\n",
    "df = df.drop_duplicates(subset=[‘A’])\t\t # drops duplicates \n",
    "df = df.dropna()    \t\t\t\t # drops NaN values\n",
    "df.dropna(‘col_name’, axis=1, inplace=True) \t # drops missing data column, axis = 0 for rows\n",
    "df = df.drop([‘col1_name’, ‘col2_name’’], axis=1)\t # drops columns/rows\n",
    "df.count()\t\t\t\t\t # count row/col\n",
    "# SimpleImputer() to replace NaN\n",
    "from sklearn.impute import SimpleImputer\n",
    "imputer = SimpleImputer(strategy ='mean')\n",
    "df_imputed = pd.DataFrame(imputer.fit_transform(df), columns=df.columns) # fit & transform\n",
    "# combine DataFrames\n",
    "combined_df = pd.concat(df, ignore_index=True)\n",
    "combined_df.to_csv('combined.csv', index=False)\n",
    "# Places columns into categorical and numerical dictionary\n",
    "# separate num and obj\n",
    "cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "# dictionary of num and obj\n",
    "columns_dict = {\n",
    "    'Categorical Columns': cat_cols,\n",
    "    'Numerical Columns': num_cols\n",
    "}\n",
    "# detect outliers\n",
    "# Function to find outliers using the IQR method\n",
    "def find_outliers(df):\n",
    "    Q1 = df.quantile(0.25)\n",
    "    Q3 = df.quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5 * IQR\n",
    "    upper_bound = Q3 + 1.5 * IQR\n",
    "    lower_outliers = df < lower_bound\n",
    "    upper_outliers = df > upper_bound\n",
    "    combined_outliers = lower_outliers | upper_outliers\n",
    "    return lower_outliers, upper_outliers, combined_outliers, lower_bound, upper_bound\n",
    "\n",
    "# Find outliers\n",
    "lower_outliers, upper_outliers, combined_outliers, lower_bound, upper_bound = find_outliers(df)\n",
    "\n",
    "# Count number of outliers for each numerical column\n",
    "lower_outlier_counts = lower_outliers.sum()\n",
    "upper_outlier_counts = upper_outliers.sum()\n",
    "combined_outlier_counts = combined_outliers.sum()\n",
    "\n",
    "# Create a DataFrame to display all outlier counts and ranges together\n",
    "outliers_summary = pd.DataFrame({\n",
    "    'Lower Outliers': lower_outlier_counts,\n",
    "    'Upper Outliers': upper_outlier_counts,\n",
    "    'Combined Outliers': combined_outlier_counts,\n",
    "    'Lower Bound': lower_bound,\n",
    "    'Upper Bound': upper_bound\n",
    "})\n",
    "\n",
    "# Print the summary table\n",
    "print(outliers_summary.T)\n",
    "# Grouping Data\n",
    "sales.groupby('type')['sold'].agg([max, sum])\n",
    "category_sentiment_counts = merge_df.groupby('Category')['Sentiment'].value_counts().unstack()\n",
    "# cross tabulation between categoricals (table of similarity)\n",
    "crosstab = pd.crosstab(df[cat1], df['cat2'])\n",
    "\n",
    "# display univariate using boxplot and histplot \n",
    "def plot_numerical_columns(df):\n",
    "    # Separate columns into categorical and numerical\n",
    "    # cat_cols = df.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "    # num_cols = df.select_dtypes(include=['number']).columns.tolist()\n",
    "\n",
    "    # Plotting each numerical column\n",
    "    for i in range(0, len(num_cols), 2):\n",
    "        plt.figure(figsize=(16, 6))\n",
    "\n",
    "        # Histogram for first numerical column\n",
    "        plt.subplot(1, 2, 1)\n",
    "        sns.histplot(data=df, x=num_cols[i], kde=True)\n",
    "        plt.title(f'Histogram of {num_cols[i]}')\n",
    "\n",
    "        # Box plot for first numerical column\n",
    "        plt.subplot(1, 2, 2)\n",
    "        sns.boxplot(data=df, x=num_cols[i])\n",
    "        plt.title(f'Boxplot of {num_cols[i]}')\n",
    "\n",
    "        # Check if there is a second numerical column to plot\n",
    "        if i + 1 < len(num_cols):\n",
    "            # Histogram for second numerical column\n",
    "            plt.figure(figsize=(16, 6))\n",
    "            plt.subplot(1, 2, 1)\n",
    "            sns.histplot(data=df, x=num_cols[i + 1], kde=True)\n",
    "            plt.title(f'Histogram of {num_cols[i + 1]}')\n",
    "\n",
    "            # Box plot for second numerical column\n",
    "            plt.subplot(1, 2, 2)\n",
    "            sns.boxplot(data=df, x=num_cols[i + 1])\n",
    "            plt.title(f'Boxplot of {num_cols[i + 1]}')\n",
    "\n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "plot_numerical_columns(df)\n",
    "# Bins, countplot\n",
    "sns.countplot(data=df, y='crop') \t# sns.countplot for categorical 'crop' column\n",
    "~~~~~~~\n",
    "plt.figure(figsize=(8,6))\n",
    "# Convert 'Age' to categorical bins\n",
    "df['Age_Bins'] = pd.cut(df['Age'], bins=range(0, 101, 5), right=False)\n",
    "\n",
    "# Plot the barplot with the binned ages\n",
    "sns.barplot(data=df, x='Age_Bins', y='Personal_Loan')\n",
    "plt.title('Personal Loan and Age')\n",
    "plt.xticks(rotation=90);\n",
    "~~~~~~\n",
    "plt.figure(figsize=(8,6))\n",
    "df['Education_Bins'] = df['Education'].map({1: 'Undergrad', 2: 'Graduate', 3: 'Advanced/Professional'})\n",
    "sns.barplot(data = df, x = 'Education_Bins', y = 'Personal_Loan')\n",
    "\n",
    "\n",
    "\n",
    "# display dictionary of bivariate (focus on visualizing target if there is)\n",
    "sns.pairplot(data = df[num_cols])\t# grid of scatterplot\n",
    "# heat map\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(df[num_cols].corr(), annot=True, cmap='coolwarm') \t#numerical columns only\n",
    "plt.title('Correlation Matrix Heatmap');\n",
    "~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~Model~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~\n",
    "# classification report: table of metrics for classification (below)\n",
    "# Accuracy:\t% of correct prediction\n",
    "# Macro avg:\tavg for each class\n",
    "# Weighted avg:avg weighted by number of instances in each class\n",
    "# Precision:\tproportion of true positive among all predicted positive\n",
    "# Recall:\tproportion of true positive among all real positives\n",
    "# F1-Score:\tharmonic mean of precision and recall\n",
    "# Support:\tnumber of instances each class in the dataset\n",
    "\n",
    "# regression metrics\n",
    "# MSE:\t\t\t\n",
    "# RMSE:\n",
    "# R2:\n",
    "# Adjusted R2:\n",
    "# MAPE:\n",
    "# import sklearn library (target classification)\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, KFold\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import classification_report, confusion_matrix, accuracy_score\n",
    "import numpy as np\n",
    "\n",
    "# Separate features and target\n",
    "X = df[['Income', 'CCAvg', 'CD_Account']]\n",
    "y = df['Personal_Loan']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "\n",
    "# Identify categorical and numerical columns\n",
    "categorical_cols = X_train.select_dtypes(include=['object', 'category']).columns\n",
    "numerical_cols = X_train.select_dtypes(include=['number']).columns\n",
    "\n",
    "# Preprocessing: OneHotEncoder for categorical features, StandardScaler for numerical features\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols),\n",
    "        ('num', StandardScaler(), numerical_cols)\n",
    "    ],\n",
    "    remainder='passthrough'\n",
    ")\n",
    "\n",
    "# PCA with 5 components (adjust as needed)\n",
    "pca = PCA()\n",
    "\n",
    "# Define the models and their parameter grids\n",
    "models_and_params = {\n",
    "    'RandomForest': {\n",
    "        'model': RandomForestClassifier(random_state=42),\n",
    "        'params': {\n",
    "            'classifier__n_estimators': [50, 100, 200],\n",
    "            'classifier__max_depth': [None, 10, 20, 30],\n",
    "            'classifier__min_samples_split': [2, 5, 10],\n",
    "            'classifier__min_samples_leaf': [1, 2, 4]\n",
    "        }\n",
    "    },\n",
    "    'LogisticRegression': {\n",
    "        'model': LogisticRegression(random_state=42, max_iter=1000),\n",
    "        'params': {\n",
    "            'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "            'classifier__solver': ['lbfgs', 'liblinear']\n",
    "        }\n",
    "    },\n",
    "    'SVM': {\n",
    "        'model': SVC(random_state=42),\n",
    "        'params': {\n",
    "            'classifier__C': [0.01, 0.1, 1, 10, 100],\n",
    "            'classifier__kernel': ['linear', 'rbf']\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Function to evaluate models with cross-validation\n",
    "def evaluate_model(model, X, y):\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    cv_scores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\n",
    "    print(\"Cross-Validation Scores:\", cv_scores)\n",
    "    print(\"Mean Cross-Validation Score:\", np.mean(cv_scores))\n",
    "\n",
    "# Train and evaluate each model\n",
    "for model_name, config in models_and_params.items():\n",
    "    print(f\"--- {model_name} ---\")\n",
    "    model = config['model']\n",
    "    param_grid = config['params']\n",
    "    \n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('pca', pca),\n",
    "        ('classifier', model)\n",
    "    ])\n",
    "    \n",
    "    evaluate_model(pipeline, X_train, y_train)\n",
    "\n",
    "    # Hyperparameter tuning using GridSearchCV\n",
    "    grid_search = GridSearchCV(pipeline, param_grid, cv=5, n_jobs=-1, scoring='accuracy')\n",
    "    grid_search.fit(X_train, y_train)\n",
    "    \n",
    "    print(\"Best Parameters:\", grid_search.best_params_)\n",
    "    print(\"Best Cross-Validation Score:\", grid_search.best_score_)\n",
    "\n",
    "    # Evaluate the best model on the test set\n",
    "    best_model = grid_search.best_estimator_\n",
    "    y_pred_best = best_model.predict(X_test)\n",
    "    \n",
    "    print(\"Best Model Classification Report:\")\n",
    "    print(classification_report(y_test, y_pred_best))\n",
    "    print(\"Best Model Confusion Matrix:\")\n",
    "    print(confusion_matrix(y_test, y_pred_best))\n",
    "    print(\"Best Model Accuracy Score:\")\n",
    "    print(accuracy_score(y_test, y_pred_best))\n",
    "    print(\"\\n\")\n",
    "\n",
    "# import sklearn library (target regression) \n",
    "from sklearn.model_selection import train_test_split, GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_percentage_error\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load your dataset (ensure to load it as needed, here assuming df is your DataFrame)\n",
    "# df = pd.read_csv('your_dataset.csv')\n",
    "\n",
    "# Separate target variable 'p' and features\n",
    "X = df.drop(columns=['P'])\n",
    "y = df['P']\n",
    "\n",
    "# Identify numerical and categorical columns\n",
    "num_cols = X.select_dtypes(include=['number']).columns.tolist()\n",
    "cat_cols = X.select_dtypes(include=['object', 'category']).columns.tolist()\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Preprocessing for numerical data\n",
    "num_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='median')),\n",
    "    ('scaler', StandardScaler())\n",
    "])\n",
    "\n",
    "# Preprocessing for categorical data\n",
    "cat_transformer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "    ('onehot', OneHotEncoder(handle_unknown='ignore'))\n",
    "])\n",
    "\n",
    "# Combine preprocessing steps\n",
    "preprocessor = ColumnTransformer(\n",
    "    transformers=[\n",
    "        ('num', num_transformer, num_cols),\n",
    "        ('cat', cat_transformer, cat_cols)\n",
    "    ])\n",
    "\n",
    "# Define the models\n",
    "models = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'Random Forest': RandomForestRegressor(random_state=42),\n",
    "    'Gradient Boosting': GradientBoostingRegressor(random_state=42)\n",
    "}\n",
    "\n",
    "# Function to calculate Adjusted R-squared\n",
    "def adjusted_r2(r2, n, k):\n",
    "    return 1 - ((1 - r2) * (n - 1) / (n - k - 1))\n",
    "\n",
    "# Train and evaluate each model\n",
    "results = {}\n",
    "\n",
    "for model_name, model in models.items():\n",
    "    pipeline = Pipeline(steps=[\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('model', model)\n",
    "    ])\n",
    "    \n",
    "    # Train the model\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    \n",
    "    # Make predictions\n",
    "    y_pred_train = pipeline.predict(X_train)\n",
    "    y_pred_test = pipeline.predict(X_test)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    mse_train = mean_squared_error(y_train, y_pred_train)\n",
    "    mse_test = mean_squared_error(y_test, y_pred_test)\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "    r2_train = r2_score(y_train, y_pred_train)\n",
    "    r2_test = r2_score(y_test, y_pred_test)\n",
    "    adj_r2_train = adjusted_r2(r2_train, X_train.shape[0], X_train.shape[1])\n",
    "    adj_r2_test = adjusted_r2(r2_test, X_test.shape[0], X_test.shape[1])\n",
    "    mape_train = mean_absolute_percentage_error(y_train, y_pred_train)\n",
    "    mape_test = mean_absolute_percentage_error(y_test, y_pred_test)\n",
    "    \n",
    "    results[model_name] = {\n",
    "        'Train MSE': mse_train,\n",
    "        'Test MSE': mse_test,\n",
    "        'Train RMSE': rmse_train,\n",
    "        'Test RMSE': rmse_test,\n",
    "        'Train R2': r2_train,\n",
    "        'Test R2': r2_test,\n",
    "        'Train Adjusted R2': adj_r2_train,\n",
    "        'Test Adjusted R2': adj_r2_test,\n",
    "        'Train MAPE': mape_train,\n",
    "        'Test MAPE': mape_test,\n",
    "    }\n",
    "\n",
    "# Display the results\n",
    "results_df = pd.DataFrame(results) # add '.T' to transpose\n",
    "print(results_df)\n",
    "\n",
    "# Visualize the feature importances for the Random Forest model (if applicable)\n",
    "best_model = models['Random Forest']\n",
    "best_pipeline = Pipeline(steps=[\n",
    "    ('preprocessor', preprocessor),\n",
    "    ('model', best_model)\n",
    "])\n",
    "\n",
    "best_pipeline.fit(X_train, y_train)\n",
    "importances = best_model.feature_importances_\n",
    "features = num_cols + list(best_pipeline.named_steps['preprocessor'].transformers_[1][1].named_steps['onehot'].get_feature_names_out(cat_cols))\n",
    "feature_importance_df = pd.DataFrame({'Feature': features, 'Importance': importances})\n",
    "feature_importance_df = feature_importance_df.sort_values(by='Importance', ascending=False)\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.barplot(x='Importance', y='Feature', data=feature_importance_df)\n",
    "plt.title('Feature Importances for Random Forest')\n",
    "plt.show()\n",
    "\n",
    "# Documentation and Reporting\n",
    "\n",
    "1. **Standardize Your Data**: PCA is sensitive to the scale of the data. Always standardize or normalize your features before applying PCA.\n",
    "    ```python\n",
    "    from sklearn.preprocessing import StandardScaler\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "    ```\n",
    "\n",
    "2. **Choose the Right Number of Components**: Use explained variance ratio to decide the number of principal components. Aim to keep 95-99% of the variance.\n",
    "    ```python\n",
    "    pca = PCA()\n",
    "    pca.fit(X_scaled)\n",
    "    explained_variance = np.cumsum(pca.explained_variance_ratio_)\n",
    "    plt.plot(explained_variance)\n",
    "    plt.xlabel('Number of Components')\n",
    "    plt.ylabel('Explained Variance')\n",
    "    plt.title('Explained Variance vs Number of Components')\n",
    "    plt.show()\n",
    "    ```\n",
    "\n",
    "3. **Interpreting PCA Components**: Principal components are linear combinations of the original features. Interpretation might be difficult, but understanding which features contribute most to each component can give insights.\n",
    "    ```python\n",
    "    pd.DataFrame(pca.components_, columns=X.columns, index=[f'PC{i+1}' for i in range(pca.n_components_)])\n",
    "    ```\n",
    "\n",
    "4. **Avoid Over-Reduction**: Reducing dimensions too much can lead to loss of significant information. Balance between dimensionality reduction and preserving variance.\n",
    "    ```python\n",
    "    pca = PCA(n_components=5)  # Select a reasonable number of components\n",
    "    X_pca = pca.fit_transform(X_scaled)\n",
    "    ```\n",
    "\n",
    "5. **Model Compatibility**: PCA-transformed data can be used with most models, but ensure the assumptions of the models are still met with the transformed data. For example, linear models might benefit from PCA by handling multicollinearity, but tree-based models (like Random Forest) might not gain as much since they can handle multicollinearity naturally.\n",
    "\n",
    "6. **Cross-Validation**: Use cross-validation to ensure the robustness of the PCA and model pipeline.\n",
    "    ```python\n",
    "    from sklearn.model_selection import cross_val_score\n",
    "    scores = cross_val_score(model, X_pca, y, cv=5)\n",
    "    print(\"Cross-Validation Scores:\", scores)\n",
    "    ```\n",
    "\n",
    "7. **Pipeline Integration**: Integrate PCA in a pipeline to streamline the process and avoid data leakage.\n",
    "    ```python\n",
    "    from sklearn.pipeline import Pipeline\n",
    "    from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "    pipeline = Pipeline([\n",
    "        ('scaler', StandardScaler()),\n",
    "        ('pca', PCA(n_components=5)),\n",
    "        ('model', RandomForestClassifier())\n",
    "    ])\n",
    "\n",
    "    pipeline.fit(X_train, y_train)\n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    ```\n",
    "\n",
    "By following these steps and considerations, you can effectively incorporate PCA into your machine learning workflow and improve the performance of your models.\n",
    "\n",
    "\n",
    "\n",
    "Complete Machine Learning Steps From ChatGPT 3.5 and recently released limited free 4.os (steps created 5/22/24)\n",
    "Problem Definition:\n",
    "Clearly define the problem you want to solve. Understand the business context and the specific question you need to answer using ML.\n",
    "Data Collection:\n",
    "Gather relevant data from various sources. Ensure that the data is representative of the problem you're addressing.\n",
    "Data Exploration:\n",
    "Perform exploratory data analysis (EDA) to understand the data's structure, patterns, and relationships. Use visualizations and statistical methods.\n",
    "Data Cleaning:\n",
    "Handle missing values, outliers, and inconsistencies. Ensure data quality by correcting errors and standardizing formats.\n",
    "Feature Engineering:\n",
    "Create new features from existing data that can help the model learn better. This includes transformations, aggregations, and encoding categorical variables.\n",
    "Data Splitting:\n",
    "Split the data into training and testing sets. Typically, an 80-20 split is used, but this can vary based on the dataset size and problem.\n",
    "Model Selection:\n",
    "Choose appropriate algorithms for your problem (e.g., regression, classification, clustering). Consider simplicity, interpretability, and performance.\n",
    "Model Training:\n",
    "Train the model on the training dataset. Optimize the model parameters to improve its performance.\n",
    "Model Evaluation:\n",
    "Evaluate the model's performance using metrics appropriate for your problem (e.g., accuracy, precision, recall, F1 score for classification). Use the testing set for this purpose.\n",
    "Model Tuning:\n",
    "Fine-tune hyperparameters using techniques like grid search or random search. Cross-validation helps ensure the model's robustness.\n",
    "Model Interpretation:\n",
    "Understand and interpret the model's predictions. Use techniques like feature importance and SHAP values to explain the model.\n",
    "Model Deployment:\n",
    "Deploy the model to a production environment where it can start making predictions on new data. Ensure it integrates well with existing systems.\n",
    "Monitoring and Maintenance:\n",
    "Continuously monitor the model's performance over time. Retrain the model as necessary to handle new data and changing patterns.\n",
    "Documentation and Reporting:\n",
    "Document the entire process, including data sources, preprocessing steps, model choices, and performance metrics. Prepare reports for stakeholders.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
